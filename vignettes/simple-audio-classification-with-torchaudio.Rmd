---
title: "Simple Audio Classification with Torchaudio"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Simple Audio Classification with Torchaudio}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This article atempts to translate the [Daniel Falbel's](https://github.com/dfalbel) ['Simple Audio Classification'](https://blogs.rstudio.com/ai/posts/2018-06-06-simple-audio-classification-keras/) article from `{tensorflow}+ {keras}` to `{torch}+{torchaudio}`.

<!-- https://towardsdatascience.com/a-tale-of-two-frameworks-985fa7fcec#6f89 -->

```{r setup}
library(torchaudio)
library(tidyverse)
```

# Downloading and Importing

```{r, eval = FALSE}
speechcommand_df <- speechcommand_dataset("..", download = TRUE)
```

```{r}
# Importing
files <- fs::dir_ls(
  path = "data/speech_commands_v0.01/", 
  recurse = TRUE, 
  glob = "*.wav"
)

files <- files[!str_detect(files, "background_noise")]

df <- tibble(
  fname = files, 
  class = fname %>% str_extract("1/.*/") %>% 
    str_replace_all("1/", "") %>%
    str_replace_all("/", ""),
  class_id = class %>% as.factor() %>% as.integer() - 1L
)
```

# Generator

```{r}
# in miliseconds
window_size_ms <- 30
window_stride_ms <- 10

# in samples
sample_rate <- 16000
window_size <- as.integer(sample_rate*window_size_ms/1000)
stride <- as.integer(sample_rate*window_stride_ms/1000)

# FFT frequency bins
fft_size <- as.integer(2^trunc(log(window_size, 2)) + 1)
n_chunks <- length(seq(window_size/2, sample_rate - window_size/2, stride))
```

Now is time to create the pre-processing function for each audio file. To recall, this function will: 1) read the raw audio file form disk to memory and calculate its spectrogram along with the respective encoded response vector.

```{r}

```

```{r}
speech_command_train = speech_command_dataset(df$fname)
```

```{r}
speech_command_train$get_item()
```

